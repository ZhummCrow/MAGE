{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504396ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from urllib import request\n",
    "from zeep import Client\n",
    "import hashlib\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import ast\n",
    "import pubchempy as pcp\n",
    "from Bio import Entrez\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f1b4ae",
   "metadata": {},
   "source": [
    "# 1.Downloading origin data from BRENDA\n",
    "Downloading data from BRENDA can take a couple of hours. To download the data from BRENDA, a registration is needed (https://www.brenda-enzymes.org/register.php). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsdl = \"https://www.brenda-enzymes.org/soap/brenda_zeep.wsdl\"\n",
    "\n",
    "email = \"your email address\" # register in https://www.brenda-enzymes.org/\n",
    "brenda_password='password'\n",
    "password = hashlib.sha256(brenda_password.encode(\"utf-8\")).hexdigest()\n",
    "client = Client(wsdl)\n",
    "parameters = (email,password)\n",
    "EC_numbers = client.service.getEcNumbersFromEcNumber(*parameters)\n",
    "print(\"There exist %s different EC numbers in the BRENDA database.\" % len(EC_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56eac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_data_save_path = \"./brenda_data_cache/EC_number_web_result/\"\n",
    "os.makedirs(web_data_save_path,exist_ok=True)\n",
    "for ec in EC_numbers:\n",
    "    url = f\"https://www.brenda-enzymes.org/enzyme.php?ecno={ec}#TURNOVER%20NUMBER%20[1/s]\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code==200:\n",
    "        with open(web_data_save_path+f\"{ec}.web\",'w') as f:\n",
    "            f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_kcat_km(target):\n",
    "    dataset = []\n",
    "    target_DivId = 'tab44' if target=='kcat' else 'tab12'\n",
    "    for EC_web_file_name in tqdm(os.listdir(web_data_save_path)):\n",
    "        file = open(web_data_save_path+EC_web_file_name,'r').read()\n",
    "\n",
    "        EC_number = EC_web_file_name.split('.web')[0]\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "        table_div = soup.find('div', id=target_DivId)\n",
    "        # Make sure to find the header\n",
    "        if table_div:\n",
    "            # Get the table row, the subsequent rows of the table will be obtained from id='tab44/tab12'\n",
    "            table_rows = table_div.find_all('div', recursive=False)\n",
    "            for row in table_rows:\n",
    "                cells = row.find_all('div', class_='cell')\n",
    "                # Extract the contents of each cell\n",
    "                if len(cells) == 7:  # Make sure the row has 7 columns of data\n",
    "                    kinetic = cells[0].text.strip()  # kcat/km value\n",
    "                    substrate = cells[1].text.strip()  \n",
    "                    organism = cells[2].text.strip()  \n",
    "                    uniprot = cells[3].text.strip()  # UniProt ID\n",
    "                    commentary = cells[4].text.strip().lower()  \n",
    "                    literature = cells[5].text.strip()  # Literature number\n",
    "                    if 'entries' in organism:\n",
    "                        continue\n",
    "                    row_data = [EC_number,organism, uniprot,literature,substrate, kinetic, commentary]\n",
    "                    dataset.append(row_data)\n",
    "\n",
    "        else:\n",
    "            print(EC_number)\n",
    "    return dataset\n",
    "\n",
    "kcat_dataset = process_kcat_km('kcat')\n",
    "km_dataset = process_kcat_km('km')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c508a1",
   "metadata": {},
   "source": [
    "# 2. Data cleaning\n",
    "Get smiles and clean up missing entries or abnormal data. Downloading data from pubchem can take a couple of hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be440ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(kcat_dataset))\n",
    "kcat_dataset = [row for row in kcat_dataset if row[5] != 'additional information']\n",
    "kcat_dataset = [row for row in kcat_dataset if 'entries' not in row[1]]\n",
    "kcat_dataset = [row for row in kcat_dataset if len(row[2].split(';'))==1] # multi chain\n",
    "kcat_dataset = [row for row in kcat_dataset if len(row[2].split(','))==1] # multi chain\n",
    "kcat_dataset = [row for row in kcat_dataset if ('-' not in row[5]) and (float(row[5])<100000)]\n",
    "kcat_dataset = [row for row in kcat_dataset if ('-' not in row[5]) and (float(row[5])>0.00001)]\n",
    "print(len(kcat_dataset))\n",
    "\n",
    "print(len(km_dataset))\n",
    "km_dataset = [row for row in km_dataset if row[5] != 'additional information']\n",
    "km_dataset = [row for row in km_dataset if 'entries' not in row[1]]\n",
    "km_dataset = [row for row in km_dataset if len(row[2].split(';'))==1]\n",
    "km_dataset = [row for row in km_dataset if len(row[2].split(','))==1]\n",
    "km_dataset = [row for row in km_dataset if ('-' not in row[5]) and (float(row[5])<100000)]\n",
    "km_dataset = [row for row in km_dataset if ('-' not in row[5]) and (float(row[5])>0.00001)]\n",
    "print(len(km_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete data for which SMILES cannot be obtained based on the substrate or for which the SMILES format is abnormal.\n",
    "subs = list(set([row[4] for row in kcat_dataset]+[row[4] for row in km_dataset]))\n",
    "smiles_dict={}\n",
    "for sub in tqdm(subs):\n",
    "    comp=get_comp(sub)\n",
    "    if comp == -1: # retry  if failed\n",
    "        comp=get_comp(sub)\n",
    "    if comp == -1:\n",
    "        smiles_dict[sub] = -1\n",
    "        continue\n",
    "    smiles = comp.canonical_smiles\n",
    "    smiles_dict[sub]=-1 if not smiles or '.' in smiles else comp\n",
    "pickle.dump(smiles_dict,open(\"./brenda_data_cache/smiles_dict.pkl\",'wb'))\n",
    "\n",
    "kcat_dataset = [row for row in kcat_dataset if smiles_dict[row[4]]!=-1]\n",
    "km_dataset = [row for row in km_dataset if smiles_dict[row[4]]!=-1]\n",
    "print(len(kcat_dataset))\n",
    "print(len(km_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean comments by normalizing stray Windows-1252 characters\n",
    "kcat_dataset_clean = sanitize_column_text(kcat_dataset)\n",
    "km_dataset_clean = sanitize_column_text(km_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ea806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91a41358",
   "metadata": {},
   "source": [
    "# 3. Parse experimental conditions\n",
    "Extract pH, temperature, cosubstrate and buffer from the **comments** using regular expressions and LLM, then perform an initial clustering.\n",
    "\n",
    "Cluster records that share the same **EC number, species, UniProt ID, substrate, reference, temperature, pH, cosubstrate and buffer**. Then, discard clusters that contain neither wild-type nor mutant entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d4660",
   "metadata": {},
   "source": [
    "#### (a) Extract pH and temperature fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f01bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kcat_dataset_with_ph_temp = Add_temperature_pH_fieds(kcat_dataset_clean)\n",
    "km_dataset_with_ph_temp = Add_temperature_pH_fieds(km_dataset_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea2590",
   "metadata": {},
   "source": [
    "#### (b) Initial clustering\n",
    "The initial clustering is intended to exclude unreasonable clusters and reduce the workload of subsequent LLM extraction and manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded8b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(dataset):\n",
    "    cluster_dict={}\n",
    "    for row in dataset:\n",
    "        pair_name = ';;;'.join([row[0],row[1],row[2],row[3],row[4],row[7],row[8]])\n",
    "        if pair_name not in cluster_dict:\n",
    "            cluster_dict[pair_name]=[]\n",
    "        cluster_dict[pair_name].append(row)\n",
    "    print(\"Same EC number, species, uniprotid, substrate, reference, temperature, pH, cluster: \",len(cluster_dict))\n",
    "\n",
    "    for pair_name in list(cluster_dict.keys()):\n",
    "        rows = cluster_dict[pair_name]\n",
    "        # Keep only rows that are wild-type or have at least one extracted mutation; drop everything else\n",
    "        new_rows = [] \n",
    "        for row in rows:\n",
    "            mutant_extracted = extract_mutations(row[6])\n",
    "            is_wildtype = 'wild' in row[6]\n",
    "            if (len(mutant_extracted)==0 and not is_wildtype) or (len(mutant_extracted)!=0 and is_wildtype):\n",
    "                continue\n",
    "            new_rows.append(row)\n",
    "        cluster_dict[pair_name]=new_rows\n",
    "        \n",
    "        # Extract the description fragment of cosubstrate and further subdivide the cluster\n",
    "        comments=[row[6] for row in new_rows]\n",
    "        cosub_extracted = [[i for i in c.split(',') if 'cosubstrate' in i or 'co-substrate' in i][0] if 'cosubstrate' in c or 'co-substrate' in c else -1 for c in comments]\n",
    "        if len(set(cosub_extracted))<=1: # All are the same cosub\n",
    "            continue\n",
    "        cluster_dict.pop(pair_name) \n",
    "        for cosub_info in set(cosub_extracted):\n",
    "            new_pair_name = pair_name+\";;;\"+str(cosub_info)\n",
    "            cluster_dict[new_pair_name]=[]\n",
    "        for cosub_info,row in zip(cosub_extracted,new_rows):\n",
    "            cluster_dict[pair_name+\";;;\"+str(cosub_info)].append(row)\n",
    "                        \n",
    "    # Delete clusters without wildtype or mutant\n",
    "    for pair_name in list(cluster_dict.keys()):\n",
    "        new_rows = cluster_dict[pair_name]\n",
    "        wildtype = [1 for row in new_rows if 'wild' in row[6]]\n",
    "        muttype = [1 for row in new_rows if len(extract_mutations(row[6]))>0]\n",
    "        if len(new_rows)<2 or sum(wildtype)==0 or sum(muttype)==0:\n",
    "            cluster_dict.pop(pair_name)\n",
    "    print(\"remain cluster: \",len(cluster_dict))\n",
    "\n",
    "    return cluster_dict\n",
    "\n",
    "\n",
    "kcat_cluster_init = get_cluster(kcat_dataset_with_ph_temp)\n",
    "km_cluster_init = get_cluster(km_dataset_with_ph_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab3b9e",
   "metadata": {},
   "source": [
    "#### (c) Extract buffer information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ab8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [row[6] for _,rows in kcat_cluster_init.items() for row in rows]\n",
    "comments += [row[6] for _,rows in km_cluster_init.items() for row in rows]\n",
    "comments = list(set(comments))\n",
    "\n",
    "with open(\"./brenda_data_cache/comment.txt\",'w') as f:\n",
    "    for com in comments:\n",
    "        f.write(com+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb66495",
   "metadata": {},
   "source": [
    "Use an LLM and manual review on the saved `comment.txt` to extract buffer information from each comment, producing a dictionary mapping comment to buffer information. Then use this dictionary to further subdivide the existing clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_split_cluster(cluster_dict, target):\n",
    "    buffer_dict = pickle.load(open(\"./brenda_data_cache/buffer_mapping.pkl\",'rb'))\n",
    "    for pair_name in cluster_dict:\n",
    "        rows = cluster_dict[pair_name]\n",
    "        for i in range(len(rows)):\n",
    "            print(len(rows[i]))\n",
    "            rows[i].append(buffer_dict[rows[i][6]])\n",
    "        cluster_dict[pair_name] = rows\n",
    "    \n",
    "    for pair_name in list(cluster_dict.keys()):\n",
    "        rows = cluster_dict[pair_name]\n",
    "        buffers = [row[9] for row in rows]\n",
    "        # All data have the same buffer\n",
    "        if len(set(buffers))==1:\n",
    "            continue\n",
    "        # There are different buffers in a cluster\n",
    "        cluster_dict.pop(pair_name)\n",
    "        for buffer in set(buffers):\n",
    "            new_pair_name = pair_name+\";;;\"+str(buffer)\n",
    "            new_rows = [row for row in rows if row[9] == buffer]\n",
    "            type = [1 if 'wild' in row[6] else 0 for row in sub_grouped_buffer_rows]\n",
    "            if type.count(1)==0 or type.count(0)==0: # exclude lacking wildtype or mutant cluster\n",
    "                continue\n",
    "            cluster_dict[new_pair_name] = new_rows\n",
    "    return cluster_dict\n",
    "kcat_cluster = buffer_split_cluster(kcat_cluster_init,\"kcat\")\n",
    "km_cluster = buffer_split_cluster(km_cluster,'km')\n",
    "len(kcat_cluster),len(km_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81fb6b",
   "metadata": {},
   "source": [
    "# 4. Enzyme information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76302f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "kcat_cluster = pickle.load(open(\"./brenda_data_cache/06kcat_cluster.pkl\",'rb'))\n",
    "km_cluster = pickle.load(open(\"./brenda_data_cache/06km_cluster.pkl\",'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00590408",
   "metadata": {},
   "source": [
    "#### (a) Download Uniprot ID by EC number and species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3a5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = \"dkroundz@gmail.com\"\n",
    "def get_taxonomy_id(organism_name):\n",
    "    # Search the Taxonomy ID by species name\n",
    "    max_attempts = 5\n",
    "    res = -1 \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            handle = Entrez.esearch(db=\"taxonomy\", term=organism_name)\n",
    "            record = Entrez.read(handle)\n",
    "            handle.close()\n",
    "\n",
    "            if record[\"IdList\"]:\n",
    "                tax_id = record[\"IdList\"][0]  \n",
    "                handle = Entrez.efetch(db=\"taxonomy\", id=tax_id, retmode=\"xml\")\n",
    "                if handle:\n",
    "                    records = Entrez.read(handle)\n",
    "                    lineage = records[0][\"Lineage\"]  \n",
    "                    res = lineage\n",
    "                handle.close()\n",
    "            break\n",
    "        except:\n",
    "            if attempt == max_attempts-1:\n",
    "                print(organism_name)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: EC number; 1: species\n",
    "ec_organism_pair = {\";;;\".join([rows[0][0],rows[0][1]]):-1  for rows in list(kcat_cluster.values())+list(km_cluster.values()) if rows[0][2] == '-'}\n",
    "\n",
    "# Construct the BRENDA client in step 1.\n",
    "for ec_org in tqdm(list(ec_organism_pair.keys())):\n",
    "    max_attempts = 5\n",
    "    for attempt in range(max_attempts): \n",
    "        try:\n",
    "            ec,org = ec_org.split(\";;;\")\n",
    "            parameters = (email,password,f\"ecNumber*{ec}\", \"sequence*\",\"noOfAminoAcids*\", \"firstAccessionCode*\",\"source*\",\"id*\",\n",
    "                        f\"organism*{org}\")\n",
    "            sequence = client.service.getSequence(*parameters)\n",
    "            break\n",
    "        except Exception:\n",
    "            if attempt == max_attempts - 1:\n",
    "                print(f\"Attempt {attempt + 1} times. Exception: {Exception}\")\n",
    "                sequence = -1  \n",
    "    if sequence==-1 or len(list(sequence))!=1:\n",
    "        ec_organism_pair.pop(ec_org)\n",
    "        continue\n",
    "    \n",
    "    # Check if the species is bacteria\n",
    "    organism_name = ec_org.split(\";;;\")[1]  \n",
    "    lineage = get_taxonomy_id(organism_name)\n",
    "    if lineage and ('Bacteria' in lineage or 'bacteria' in lineage):\n",
    "        ec_organism_pair[ec_org]= list(sequence)[0]\n",
    "    else:\n",
    "        ec_organism_pair.pop(ec_org)\n",
    "len(ec_organism_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fill in the missing Uniprot IDs and discard those without IDs\n",
    "def add_uniprotId(cluster_dict):\n",
    "    for pair_name in list(cluster_dict.keys()):\n",
    "        rows = cluster_dict[pair_name]\n",
    "        uniprot = rows[0][2]\n",
    "        ec_org = \";;;\".join([rows[0][0],rows[0][1]])\n",
    "        if uniprot != '-':\n",
    "            continue\n",
    "        if ec_org not in ec_organism_pair:\n",
    "            cluster_dict.pop(pair_name)\n",
    "            continue\n",
    "        new_uniprot = ec_organism_pair[ec_org][0]['firstAccessionCode']\n",
    "        for i in range(len(rows)):\n",
    "            rows[i][2]=new_uniprot\n",
    "        cluster_dict[pair_name]=rows\n",
    "    return cluster_dict\n",
    "\n",
    "kcat_cluster = add_uniprotId(kcat_cluster)\n",
    "km_cluster = add_uniprotId(km_cluster)\n",
    "len(kcat_cluster),len(km_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761bd90",
   "metadata": {},
   "source": [
    "#### (b) Download sequences and verify mutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7da99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "UIDs = list(set([rows[0][2] for _,rows in kcat_cluster.items()] + [rows[0][2] for _,rows in km_cluster.items()]))\n",
    "UID_Seq_dict = dict()\n",
    "for id in tqdm(UIDs):\n",
    "    url = \"https://www.uniprot.org/uniprot/%s.fasta\" % id\n",
    "    try :\n",
    "        data = request.urlopen(url)\n",
    "        respdata = data.read().decode(\"utf-8\").strip()\n",
    "        seq = ''.join([i for i in respdata.split('\\n')[1:]])\n",
    "        UID_Seq_dict[id] =  seq\n",
    "    except :\n",
    "        print(id, \"can not find from uniprot!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_aa = ['U','O','X','B','J','Z']\n",
    "def check_aa(seq):\n",
    "    for aa in seq:\n",
    "        if aa in error_aa:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def check_mutant(cluster_dict):\n",
    "    for pair_name in list(cluster_dict.keys()):\n",
    "        rows = cluster_dict[pair_name]\n",
    "        uniprotId = rows[0][2]\n",
    "        seq = UID_Seq_dict[uniprotId]\n",
    "        if not check_aa(seq):\n",
    "            cluster_dict.pop(pair_name)\n",
    "            \n",
    "        \n",
    "        wt_rows = [row for row in rows  if len(extract_mutations(row[6]))==0]\n",
    "        mut_rows = [row for row in rows  if len(extract_mutations(row[6]))!=0]\n",
    "        mut_rows_new = []\n",
    "        for row in mut_rows:\n",
    "            flag=True\n",
    "            mut_loc = extract_mutations(row[6])\n",
    "            for mut in mut_loc:\n",
    "                loc = int(mut[1:-1])\n",
    "                if loc >= len(seq)-1 or mut[0] not in [seq[loc-1],seq[loc],seq[loc+1]]: \n",
    "                    flag=False\n",
    "                    break\n",
    "            if flag:\n",
    "                # If it is a multiple mutation, it is required to be at the position of +1-1 or 0\n",
    "                all_flag=False\n",
    "                for dev in range(-1,2,1):\n",
    "                    flag=True\n",
    "                    for mut in mut_loc:\n",
    "                        loc = int(mut[1:-1])+dev\n",
    "                        if mut[0] != seq[loc]:\n",
    "                            flag=False\n",
    "                            break\n",
    "                    if flag:\n",
    "                        all_flag=True\n",
    "                        break\n",
    "                if all_flag: \n",
    "                    mut_rows_new.append(row)\n",
    "        if len(mut_rows_new)==0: \n",
    "            cluster_dict.pop(pair_name)\n",
    "        else:\n",
    "            cluster_dict[pair_name] = wt_rows + mut_rows_new\n",
    "    return cluster_dict\n",
    "kcat_cluster = check_mutant(kcat_cluster)\n",
    "km_cluster = check_mutant(km_cluster)\n",
    "len(kcat_cluster),len(km_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab59b3e7",
   "metadata": {},
   "source": [
    "#### (c) Deduplication of identical data\n",
    "Deduplication of multiple wildtypes or multiple identical mutant data within a cluster requires manual intervention to avoid data loss due to errors. Here, we use a method to print the comments in the cluster to a text file for manual processing when there are controversial duplicate data in the cluster. For example, some clusters may have modified suffixes that need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba71e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate(cluster_dict):\n",
    "    for pair_name in cluster_dict:\n",
    "        wt_rows = [row for row in cluster_dict[pair_name] if 'wild' in row[6]]\n",
    "        mut_rows = [row for row in cluster_dict[pair_name] if len(extract_mutations(row[6]))>0]\n",
    "        # wt\n",
    "        if len(wt_rows)!=1:\n",
    "            K = \",\".join([row[5] for row in wt_rows])\n",
    "            wt_rows[0][5] = K\n",
    "        wt_row = wt_rows[0]\n",
    "        # mut\n",
    "        mut_locs = [\",\".join(extract_mutations(row[6])) for row in mut_rows]\n",
    "        if len(set(mut_locs))!=len(mut_locs):\n",
    "            # Put the same mutation points in the same list, \n",
    "            grouped_mut_rows = [[row for row in mut_rows if \",\".join(extract_mutations(row[6]))==mut_loc] for mut_loc in set(mut_locs)]\n",
    "            # Then put the kcat of all records in each sublist together and throw it to the first record in the sublist\n",
    "            mut_rows = []\n",
    "            for sub_grouped_mut_rows in grouped_mut_rows:\n",
    "                K = \",\".join([row[5] for row in sub_grouped_mut_rows])\n",
    "                sub_grouped_mut_rows[0][5] = K # The kcat of the first record stores all kcats of the same mutation\n",
    "                mut_rows.append(sub_grouped_mut_rows[0])\n",
    "        cluster_dict[pair_name] = [wt_row] + mut_rows\n",
    "    return cluster_dict\n",
    "    \n",
    "# The cluster here is the cluster that has been manually confirmed\n",
    "kcat_cluster = remove_duplicate(kcat_cluster)\n",
    "km_cluster = remove_duplicate(km_cluster)\n",
    "len(kcat_cluster),len(km_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a65af1",
   "metadata": {},
   "source": [
    "# 5. Construct mutation effect pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revised_mut_loc(mut_loc,seq):    \n",
    "    # If it is a multiple mutation, it is required to be at the position of +1-1 or 0\n",
    "    all_flag=False\n",
    "    true_dev = False\n",
    "    for dev in range(-1,2,1):\n",
    "        flag=True\n",
    "        for mut in mut_loc:\n",
    "            loc = int(mut[1:-1])+dev\n",
    "            if mut[0] != seq[loc]:\n",
    "                flag=False\n",
    "                break\n",
    "        if flag:\n",
    "            all_flag=True\n",
    "            true_dev=dev\n",
    "            break\n",
    "    if all_flag:\n",
    "        mut_loc_new = [mut[0]+str(int(mut[1:-1])+true_dev)+mut[-1] for mut in mut_loc]\n",
    "        return mut_loc_new\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5947cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(cluster_dict,target):\n",
    "    EcNumber = []\n",
    "    organism = []\n",
    "    substrate = []\n",
    "    UniprotId = []\n",
    "    pubmedId = []\n",
    "    temperature = []\n",
    "    pH = []\n",
    "    buffer = []\n",
    "    sequence = []\n",
    "    mutant = [] \n",
    "    wt_ks = []\n",
    "    mut_ks = []\n",
    "    delta_ks = []\n",
    "\n",
    "    for pair_name in cluster_dict:\n",
    "        wt_row = [row for row in cluster_dict[pair_name] if 'wild' in row[6]][0]\n",
    "        UID,wt_K = wt_row[2],wt_row[5]\n",
    "        wt_K = [float(i) for i in wt_row[5].split(',')]\n",
    "        wt_K = np.mean([math.log10(i) for i in wt_K])\n",
    "        seq = IdSeq_dict[UID]\n",
    "        \n",
    "        mut_rows = [row for row in cluster_dict[pair_name] if len(extract_mutations(row[6]))>0]\n",
    "        for mut_row in mut_rows:\n",
    "            mutant_info = revised_mut_loc(extract_mutations(mut_row[6]),seq)\n",
    "            if mutant_info==-1: # Dislocation mutation point\n",
    "                print(extract_mutations(mut_row[6]),seq)\n",
    "                continue\n",
    "            \n",
    "            mut_K = mut_row[5]\n",
    "            mut_K = [float(i) for i in mut_row[5].split(\",\")]\n",
    "            mut_K = np.mean([math.log10(i) for i in mut_K])\n",
    "\n",
    "            EcNumber.append(wt_row[0])\n",
    "            organism.append(wt_row[1].lower())\n",
    "            substrate.append(wt_row[4].lower())\n",
    "            UniprotId.append(UID)\n",
    "            pubmedId.append(wt_row[3])\n",
    "            temperature.append(wt_row[7])\n",
    "            pH.append(wt_row[8])\n",
    "            buffer.append(wt_row[9])\n",
    "            sequence.append(seq)\n",
    "            mutant.append(\",\".join(mutant_info))\n",
    "            wt_ks.append(wt_K)\n",
    "            mut_ks.append(mut_K)\n",
    "            delta_ks.append(mut_K-wt_K)\n",
    "    df = pd.DataFrame({\n",
    "        'EcNumber':EcNumber,'Organism':organism,\"Substrate\":substrate,\n",
    "        'UniprotId':UniprotId,'brenda_Ref_Id':pubmedId,\n",
    "        'Temperature':temperature,'pH':pH,'buffer':buffer,\n",
    "        'sequence':sequence,'mutant':mutant,\n",
    "        f'wt_{target}_log10':wt_ks,f'mut_{target}_log10':mut_ks,f'delta_{target}_log10':delta_ks,\n",
    "    })\n",
    "    return df\n",
    "kcat_df = create_df(kcat_cluster,'kcat')\n",
    "km_df = create_df(km_cluster,'km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "kcat_df_avg = kcat_df.groupby(['EcNumber', 'Organism','Substrate','UniprotId',\n",
    "                               'brenda_Ref_Id','Temperature','pH','buffer','sequence','mutant',\n",
    "                               'wt_kcat_log10','mut_kcat_log10'], as_index=False).agg({'delta_kcat_log10': 'mean'})\n",
    "km_df_avg = km_df.groupby(['EcNumber', 'Organism','Substrate','UniprotId',\n",
    "                               'brenda_Ref_Id','Temperature','pH','buffer','sequence','mutant',\n",
    "                               'wt_km_log10','mut_km_log10'], as_index=False).agg({'delta_km_log10': 'mean'})\n",
    "\n",
    "kcat_df_avg.to_csv(\"./brenda_data_cache/brenda_delta_kcat_df.csv\",index=False)\n",
    "km_df_avg.to_csv(\"./brenda_data_cache/brenda_delta_km_df.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GVP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
