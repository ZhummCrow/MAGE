{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pubchempy as pcp\n",
    "\n",
    "from urllib import request\n",
    "from utils import *\n",
    "import ast\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17b9be",
   "metadata": {},
   "source": [
    "# 1.Downloading origin data from Sabio-RK\n",
    "Downloading data from Sabio-RK can take a couple of hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ca3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_URL = 'http://sabiork.h-its.org/sabioRestWebServices/kineticlawsExportTsv'\n",
    "raw_data_save_path = \"./sabiork_data_cache/sabio_origin_file/\"\n",
    "os.makedirs(raw_data_save_path,exist_ok=True)\n",
    "file_list = os.listdir(raw_data_save_path) # Due to network problems, you can repeat it several times to ensure that all data is downloaded\n",
    "for i in range(1,10):\n",
    "    for j in range(1,500):\n",
    "        EC=f\"{i}.{j}.*\"\n",
    "        if f\"EcNumber{i}.{j}.pkl\" in file_list:\n",
    "            continue\n",
    "        query_dict = {\"ECNumber\":'%s' %EC,}\n",
    "        query_string = ' AND '.join(['%s:%s' % (k,v) for k,v in query_dict.items()])\n",
    "        query = {'fields[]':['EntryID',# The same page is used to map the substrate from km mapping\n",
    "                            'Substrate', 'ECNumber', 'Organism',\n",
    "                            'UniprotID', \n",
    "                            'EnzymeType', \n",
    "                            # PubMedID, response and buffer are used to ensure the construction of delta kcat is reasonable\n",
    "                            'PubMedID', 'SabioReactionID','KeggReactionID','Pathway', 'Buffer',\n",
    "                            'pH',\"temperature\", \n",
    "                            'Smiles',\n",
    "                            'Parameter' # kcat|km value\n",
    "                            ], 'q':query_string}\n",
    "        request = requests.post(QUERY_URL, params = query)\n",
    "        if request.status_code == 200:\n",
    "            results = [i.split(\"\\t\") for i in request.text.split(\"\\n\")]\n",
    "            if len(results)>2:\n",
    "                print(f\"EcNumber{i}.{j}.pkl\")\n",
    "                pickle.dump(results,open(raw_data_save_path+f\"EcNumber{i}.{j}.pkl\",'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f08f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65d5228e",
   "metadata": {},
   "source": [
    "# 2. Data cleaning\n",
    "#### (a) Preprocess Kcat data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kcat_entry_dict = {}\n",
    "for file in os.listdir(raw_data_save_path):\n",
    "    results = pickle.load(open(raw_data_save_path+file,'rb'))\n",
    "    for row in results[1:]:\n",
    "        if len(row)<20: # Missing data\n",
    "            continue\n",
    "        EC = row[2] \n",
    "        if sum([1 if len(i)>=1 else 0 for i in EC.split(\".\")])!=4: # # EC exception example 1.1.1. Entry id 56619\n",
    "            continue\n",
    "        # kcat has no value\n",
    "        if row[14] == 'kcat' and row[16] == '': # Entry id 69933\n",
    "            continue\n",
    "        # km substrate is not in the substrate field above\n",
    "        subs = row[1].split(';')\n",
    "        if row[14] == 'Km' and row[15] not in subs: \n",
    "            continue\n",
    "        entryId = row[0]\n",
    "        if entryId not in kcat_entry_dict:\n",
    "            kcat_entry_dict[entryId] = []\n",
    "        kcat_entry_dict[entryId].append(row)\n",
    "len(kcat_entry_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kcat_dataset = []\n",
    "for entryId,results in kcat_entry_dict.items():\n",
    "    EnzymeType = [row[14] for row in results]\n",
    "    \n",
    "    # Here we process data with only one kcat and at least one km.\n",
    "    if EnzymeType.count(\"kcat\")==1 and EnzymeType.count(\"Km\")>0: \n",
    "        kcat_row = [row for row in results if row[14]=='kcat'][0] \n",
    "        subs = [row[15] for row in results if row[14]=='Km'] \n",
    "        for sub in set(subs):\n",
    "            kcat_row_with_sub = list(kcat_row)\n",
    "            kcat_row_with_sub[15]=sub\n",
    "            kcat_dataset.append(kcat_row_with_sub)    \n",
    "            \n",
    "    # Handling data with no km, one kcat, but only one substrate.\n",
    "    elif EnzymeType.count(\"kcat\")==1 and EnzymeType.count(\"Km\")==0:\n",
    "        sub = results[0][1].split(';') # 获得所有底物\n",
    "        if len(sub) != 1:\n",
    "            continue\n",
    "        sub = sub[0]\n",
    "        kcat_row = [row for row in results if row[14]=='kcat'][0] \n",
    "        kcat_row[15]=sub[0]\n",
    "        kcat_dataset.append(kcat_row)    \n",
    "    \n",
    "    # Handling data with multiple kcat data with km entries.\n",
    "    elif EnzymeType.count(\"kcat\")>1 and EnzymeType.count(\"Km\")!=0:\n",
    "        web_results = get_data_from_sabiork(entry)\n",
    "        results = entry_dict_multi_kcat[entry]\n",
    "        subs = [row[2] for row in web_results if row[1] == 'Km']\n",
    "        kcat_rows = [row for row in results if row[14] =='kcat' and row[19] == 's^(-1)']\n",
    "\n",
    "        if len(set(subs))==1: # The case where there is only one substrate for different km.\n",
    "            sub = subs[0]\n",
    "            kcat_row = kcat_rows[0]\n",
    "            kcat_row[16]=\",\".join([row[16] for row in kcat_rows])\n",
    "            kcat_row[15]=sub\n",
    "            kcat_dataset.append(kcat_row)\n",
    "        else:               \n",
    "            if 'kcat' in [row[0] for row in web_results]: # 13 cases where name cannot be mapped. 8246\n",
    "                continue\n",
    "            Km_tail = {row[0][-1]:row[2] for row in web_results if row[1] =='Km'} # Mapping substrate\n",
    "            kcat_tail={}\n",
    "            for row in web_results:\n",
    "                if row[6] not in ['min^(-1)','s^(-1)']:\n",
    "                    continue\n",
    "                kcat = str(round(float(row[3])/60,6)) if row[6]=='min^(-1)' else row[3]\n",
    "                kcat_tail[row[0][-1]]=kcat\n",
    "            if sum([1 for tail in kcat_tail if list(Km_tail.keys()).count(tail) != 1]) != 0: # name cannot be mapped 13. cases 5316 3247\n",
    "                continue\n",
    "            for kcat_row in kcat_rows:\n",
    "                kcat_row_with_sub = list(kcat_row)\n",
    "\n",
    "                tail = [t for t,v in kcat_tail.items() if round(float(v),3) == round(float(kcat_row_with_sub[16]),3)][0]\n",
    "                sub = Km_tail[tail]\n",
    "                Km_tail.pop(tail)\n",
    "                kcat_tail.pop(tail)\n",
    "\n",
    "                kcat_row_with_sub[15]=sub\n",
    "                kcat_dataset.append(kcat_row_with_sub)\n",
    "    else:\n",
    "        continue\n",
    "len(kcat_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55994c7e",
   "metadata": {},
   "source": [
    "#### (b) Preprocess Km data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeff758",
   "metadata": {},
   "outputs": [],
   "source": [
    "km_raw_dataset = []\n",
    "filelist = os.listdir(raw_data_save_path)\n",
    "need = []\n",
    "for file in filelist:\n",
    "    results = pickle.load(open(raw_data_save_path+file,'rb'))\n",
    "    if len(results[0])==19:\n",
    "        need.append(file.split('EcNumber')[1].split('.pkl')[0])\n",
    "        continue\n",
    "    for row in results[1:]:\n",
    "        if len(row)<20:\n",
    "            continue\n",
    "        if row[14] != 'Km':\n",
    "            continue\n",
    "        if sum([1 if len(i)>=1 else 0 for i in row[2].split(\".\")])!=4: # 56619\n",
    "            continue\n",
    "        if row[14] == 'Km' and row[16] == '':  # 69733\n",
    "            continue\n",
    "        subs = row[1].split(';')\n",
    "        if row[15] not in subs:\n",
    "            continue\n",
    "        if row[19] not in ['M','mg/ml'] :\n",
    "            continue\n",
    "        \n",
    "        km_raw_dataset.append(row)\n",
    "len(km_raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630bc4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_weight_dict = {}\n",
    "for row in km_raw_dataset:\n",
    "    if row[19] == 'mg/ml':\n",
    "        mol_weight_dict[row[15]]=-1\n",
    "\n",
    "for mol in tqdm(mol_weight_dict):\n",
    "    molecular_weight = get_mol_weight(mol)\n",
    "    if molecular_weight==-1:\n",
    "        continue\n",
    "    mol_weight_dict[mol]=molecular_weight\n",
    "\n",
    "km_dataset = []\n",
    "for raw_row in km_raw_dataset:\n",
    "    row = list(raw_row)\n",
    "    if row[19] == 'mg/ml':\n",
    "        molecular_weight = mol_weight_dict[row[15]]\n",
    "        if molecular_weight == -1:\n",
    "            continue\n",
    "        km_molar = float(row[16]) / (float(molecular_weight))\n",
    "        row[16]=km_molar*1000\n",
    "    elif row[19] == 'M':\n",
    "        row[16] = float(row[16])*1000\n",
    "    row[19]='mM'\n",
    "    km_dataset.append(row)\n",
    "len(km_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186fcb78",
   "metadata": {},
   "source": [
    "#### (c) Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EC_PATTERN = re.compile(r'^[1-9]\\d*\\.(?:0|[1-9]\\d*)\\.(?:0|[1-9]\\d*)\\.(?:0|[1-9]\\d*)$')\n",
    "\n",
    "def is_valid_ec(ec):\n",
    "    if not isinstance(ec, str):\n",
    "        return False\n",
    "    ec = ec.strip()\n",
    "    return bool(EC_PATTERN.match(ec))\n",
    "\n",
    "kcat_dataset_clean=[row for row in kcat_dataset if row[4]!='' and len(row[4].split(\" \"))==1]\n",
    "kcat_dataset_clean=[row for row in kcat_dataset is_valid_ec(row[2])]\n",
    "kcat_dataset_clean=[row for row in kcat_dataset_clean if float(row[16])>0.00001 and float(row[16])<100000]\n",
    "\n",
    "km_dataset_clean=[row for row in km_dataset if row[4]!='' and len(row[4].split(\" \"))==1]\n",
    "km_dataset_clean=[row for row in km_dataset is_valid_ec(row[2])]\n",
    "km_dataset_clean=[row for row in km_dataset_clean if float(row[16])>0.00001 and float(row[16])<100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_origin_cluster_dict(dataset):\n",
    "    field_loc = [2,3,4,6,15,8,10,11,12]    \n",
    "    cluster_dict = {}\n",
    "    # Form the original cluster and exclude mutation data without mutation points\n",
    "    for row in dataset:\n",
    "        pair_name = \";;;\".join([row[i] for i in field_loc])+';;;'+\";;;\".join(sorted(row[1].split(\";\")))+';;;'+\";;;\".join(sorted(row[13].split(\";\")))\n",
    "        if row[5].startswith(\"mutant\") and len(extract_mutations(row[5]))==0:\n",
    "            continue\n",
    "        if pair_name not in cluster_dict:\n",
    "            cluster_dict[pair_name]=[]\n",
    "        cluster_dict[pair_name].append(row)\n",
    "            \n",
    "    # 排除只有wt或者mut的cluster\n",
    "    for v in list(cluster_dict.keys()):\n",
    "        rows = cluster_dict[v]\n",
    "        type = [1 if row[5].startswith('mutant') else 2 for row in rows]\n",
    "        if type.count(1)>0 and type.count(2)>0:\n",
    "            continue\n",
    "        cluster_dict.pop(v)\n",
    "\n",
    "    return cluster_dict\n",
    "kcat_cluster_dict = get_origin_cluster_dict(kcat_dataset_clean)\n",
    "km_cluster_dict = get_origin_cluster_dict(km_dataset_clean)\n",
    "print(len(kcat_cluster_dict),len(km_cluster_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccacbb0c",
   "metadata": {},
   "source": [
    "# 2. Enzyme and substrate information retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2ea2b",
   "metadata": {},
   "source": [
    "#### (a) Download SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3956342",
   "metadata": {},
   "outputs": [],
   "source": [
    "Subs = [rows[0][15] for _,rows in kcat_cluster_dict.items()] + [rows[0][15] for _,rows in km_cluster_dict.items()]\n",
    "Subs = list(set(Subs))\n",
    "smiels_dict={}\n",
    "for sub in tqdm(Subs):\n",
    "    smiels_dict[sub]=get_comp(sub)\n",
    "    if smiels_dict[sub]==-1:\n",
    "        continue\n",
    "    smiles = smiels_dict[sub].canonical_smiles\n",
    "    if '.' in smiles:\n",
    "        smiels_dict[sub]=-1\n",
    "pickle.dump(smiels_dict,open(\"./sabiork_data_cache/smiles_dict.pkl\",'wb'))\n",
    "\n",
    "\n",
    "for cluster_name in list(kcat_cluster_dict.keys()):\n",
    "    if smiels_dict[kcat_cluster_dict[cluster_name][0][15]] == -1:\n",
    "        kcat_cluster_dict.pop(cluster_name)\n",
    "for cluster_name in list(km_cluster_dict.keys()):\n",
    "    if smiels_dict[km_cluster_dict[cluster_name][0][15]] == -1:\n",
    "        km_cluster_dict.pop(cluster_name)\n",
    "len(kcat_cluster_dict),len(km_cluster_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184c0f07",
   "metadata": {},
   "source": [
    "#### (b) Download sequences and verify mutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16459806",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_aa = ['U','O','X','B','J','Z']\n",
    "def check_aa(seq):\n",
    "    for aa in seq:\n",
    "        if aa in error_aa:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "uniprotIds = list(set([rows[0][4] for _,rows in kcat_cluster_dict.items()] + [rows[0][4] for _,rows in km_cluster_dict.items()]))\n",
    "UID_Seq_dict = {}\n",
    "for id in tqdm(uniprotIds):\n",
    "    url = \"https://www.uniprot.org/uniprot/%s.fasta\" % id\n",
    "    try :\n",
    "        data = request.urlopen(url)\n",
    "        respdata = data.read().decode(\"utf-8\").strip()\n",
    "        seq = ''.join([i for i in respdata.split('\\n')[1:]])\n",
    "        if check_aa(seq):\n",
    "            UID_Seq_dict[id] = seq\n",
    "    except :\n",
    "        print(id, \"can not find from uniprot!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mut_loc(cluster_dict):\n",
    "    for pair_name in list(cluster_dict.keys()):\n",
    "        rows = cluster_dict[pair_name]\n",
    "        wt_rows = [row for row in rows if row[5].startswith(\"wildtype\")]\n",
    "        mut_rows = [row for row in rows if row[5].startswith(\"mutant\")]\n",
    "        mut_rows_right_mut_loc = []\n",
    "        for row in mut_rows:\n",
    "            mut_loc = extract_mutations(row[5])\n",
    "            seq = IdSeq_dict[row[4]]\n",
    " \n",
    "            for dev in range(-1,2,1):\n",
    "                flag=True\n",
    "                for mut in mut_loc:\n",
    "                    loc = int(mut[1:-1])+dev\n",
    "                    if loc>len(seq) or mut[0] != seq[loc]:\n",
    "                        flag=False\n",
    "                        break\n",
    "                if flag: # 全部点位都对应上了\n",
    "                    mut_rows_right_mut_loc.append(row)\n",
    "                    break\n",
    "        if len(mut_rows_right_mut_loc)==0:\n",
    "            cluster_dict.pop(pair_name)\n",
    "    return cluster_dict\n",
    "kcat_cluster_dict = check_mut_loc(kcat_cluster_dict)\n",
    "km_cluster_dict = check_mut_loc(km_cluster_dict)\n",
    "len(kcat_cluster_dict),len(km_cluster_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9617aac6",
   "metadata": {},
   "source": [
    "# 3. Construct mutation effect pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97c7c6",
   "metadata": {},
   "source": [
    "#### (a) Deduplication of identical data\n",
    "Deduplication of multiple wildtypes or multiple identical mutant data within a cluster requires manual intervention to avoid data loss due to errors. Here, we use a method to print the comments in the cluster to a text file for manual processing when there are controversial duplicate data in the cluster. For example, some clusters may have modified suffixes that need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99676379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cluster_multi_wt(cluster_dict):\n",
    "    for pair_name in list(cluster_dict.keys()):\n",
    "        rows = cluster_dict[pair_name]\n",
    "        wt_rows = [row for row in rows if row[5].startswith(\"wild\")]\n",
    "        mut_rows = [row for row in rows if row[5].startswith(\"mutant\")]\n",
    "        mut_locs = [\",\".join(extract_mutations(row[5])) for row in mut_rows]\n",
    "        \n",
    "        if len(wt_rows)>1:\n",
    "            print(wt_rows)\n",
    "            K = \",\".join([row[16] for row in wt_rows])\n",
    "            wt_rows[0][16] = K\n",
    "        wt_row = wt_rows[0]\n",
    "\n",
    "        if len(set(mut_locs))!= len(mut_locs):\n",
    "            grouped_mut_rows = [[row for row in mut_rows if \",\".join(extract_mutations(row[5]))==mut] for mut in set(mut_locs)]\n",
    "            mut_rows = []\n",
    "            for sub_grouped_mut_rows in grouped_mut_rows:\n",
    "                K = \",\".join([row[16] for row in sub_grouped_mut_rows])\n",
    "                sub_grouped_mut_rows[0][16] = K\n",
    "                mut_rows.append(sub_grouped_mut_rows[0])    \n",
    "        cluster_dict[pair_name] = [wt_row] + mut_rows\n",
    "    return cluster_dict\n",
    "    \n",
    "# The cluster here is the cluster that has been manually confirmed\n",
    "kcat_cluster_dict = remove_cluster_multi_wt(kcat_cluster_dict)\n",
    "km_cluster_dict = remove_cluster_multi_wt(km_cluster_dict)\n",
    "len(kcat_cluster_dict),len(km_cluster_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f687d2",
   "metadata": {},
   "source": [
    "#### (b) Create dataset and merge data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_pairs(cluster_dict):\n",
    "    delta_pairs = []\n",
    "    for pair_name,rows in cluster_dict.items():\n",
    "        wt_row = [row for row in rows if row[5].startswith(\"wild\")][0]\n",
    "        mut_rows = [row for row in rows if row[5].startswith(\"mutant\")]\n",
    "        uniprotId = wt_row[4]\n",
    "        seq = IdSeq_dict[uniprotId]\n",
    "        for mut_row in mut_rows:\n",
    "            mut_loc = extract_mutations(mut_row[5])\n",
    "            for dev in range(-1,2,1):\n",
    "                flag=True\n",
    "                for mut in mut_loc:\n",
    "                    loc = int(mut[1:-1])+dev\n",
    "                    if loc>=len(seq) or mut[0] != seq[loc]:\n",
    "                        flag=False\n",
    "                        break\n",
    "                if flag: \n",
    "                    mut_loc_new = [mut[0]+str(int(mut[1:-1])+dev)+mut[-1] for mut in mut_loc]\n",
    "                    delta_pairs.append([wt_row,mut_row,mut_loc_new])\n",
    "                    break\n",
    "    return delta_pairs\n",
    "kcat_delta_pairs = form_pair(kcat_cluster_dict)\n",
    "km_delta_pairs = form_pair(km_cluster_dict)\n",
    "len(kcat_delta_pairs),len(km_delta_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_df(delta_pair,target):\n",
    "    EcNumber = []\n",
    "    organism = []\n",
    "    substrate = []\n",
    "    keggReactionId = []\n",
    "    UniprotId = []\n",
    "    pubmedId = []\n",
    "    temperature = []\n",
    "    pH = []\n",
    "    buffer = []\n",
    "\n",
    "    sequence = []\n",
    "    mutant = []\n",
    "    wt_ks = []\n",
    "    mut_ks = []\n",
    "    delta_ks = []\n",
    "    wt_entry = []\n",
    "    mut_entry = []\n",
    "\n",
    "    for wt_row,mut_row,mut_loc in delta_pair:\n",
    "        wt_K,mut_K = [float(i) for i in str(wt_row[16]).split(',')],[float(i) for i in str(mut_row[16]).split(',')]\n",
    "        if target=='km':\n",
    "            wt_K = [i*1000 for i in wt_K]\n",
    "            mut_K = [i*1000 for i in mut_K]\n",
    "        count_0 = [i<=0 for i in wt_K+mut_K]\n",
    "        if sum(count_0)>0: # 出现负数\n",
    "            continue\n",
    "        wt_K = np.mean([math.log10(i) for i in wt_K])\n",
    "        mut_K = np.mean([math.log10(i) for i in mut_K])\n",
    "        \n",
    "        EcNumber.append( wt_row[2])\n",
    "        organism.append(wt_row[3].lower())\n",
    "        substrate.append(wt_row[15].lower())\n",
    "        keggReactionId.append(wt_row[8])\n",
    "        UniprotId.append(wt_row[4])\n",
    "        pubmedId.append(wt_row[6])\n",
    "        \n",
    "        temperature.append(str(float(wt_row[12])) if '-' not in wt_row[12] else '-')\n",
    "        pH.append(str(float(wt_row[11])) if '-' not in wt_row[11] else '-')\n",
    "        buffer.append(wt_row[10])\n",
    "\n",
    "        sequence.append(IdSeq_dict[wt_row[4]])\n",
    "        mutant.append(\",\".join(mut_loc))\n",
    "        wt_ks.append(wt_K)\n",
    "        mut_ks.append(mut_K)\n",
    "        delta_ks.append(mut_K-wt_K)\n",
    "        wt_entry.append(wt_row[0])\n",
    "        mut_entry.append(mut_row[0])\n",
    "    df = pd.DataFrame({\n",
    "        'EcNumber':EcNumber,'Organism':organism,\"Substrate\":substrate,\n",
    "        'KeggReactionId':keggReactionId,'UniprotId':UniprotId,'pubmedId':pubmedId,\n",
    "        'Temperature':temperature,'pH':pH,'buffer':buffer,\n",
    "        'sequence':sequence,'mutant':mutant,\n",
    "        f'wt_{target}_log10':wt_ks,f'mut_{target}_log10':mut_ks,f\"delta_{target}_log10\":delta_ks\n",
    "    })\n",
    "    return df\n",
    "delta_kcat_df = create_df(kcat_delta_pairs,'kcat')\n",
    "delta_km_df = create_df(km_delta_pairs,'km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "kcat_df_avg = delta_kcat_df.groupby(['EcNumber', 'Organism','Substrate','KeggReactionId','UniprotId',\n",
    "                               'pubmedId','Temperature','pH','buffer','sequence','mutant',\n",
    "                               'wt_kcat_log10','mut_kcat_log10'], as_index=False).agg({'delta_kcat_log10': 'mean'})\n",
    "km_df_avg = delta_km_df.groupby(['EcNumber', 'Organism','Substrate','KeggReactionId','UniprotId',\n",
    "                               'pubmedId','Temperature','pH','buffer','sequence','mutant',\n",
    "                               'wt_km_log10','mut_km_log10'], as_index=False).agg({'delta_km_log10': 'mean'})\n",
    "\n",
    "kcat_df_avg.to_csv(\"./sabiork_data_cache/sabiork_delta_kcat_df.csv\",index=False)\n",
    "km_df_avg.to_csv(\"./sabiork_data_cache/sabiork_delta_km_df.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GVP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
